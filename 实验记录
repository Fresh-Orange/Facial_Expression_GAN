2019/10/13：背景融合方式改为背景随着人脸移动，一开始使用的边缘扩展方式为边缘复制，但是存在一些测试样本的边缘是黑边，因此目前暂时换成边缘镜像

2019/10/15：
1. 数据增强版本（main_id_dataagu.py），对输入图像进行了一定的旋转，能很大程度解决重影的问题
2. kpD的SNConv版本分为两个版本进行训练。L1—loss大的版本转换效果还行就是生成的人脸“局部扭曲”，但是清晰度较“非SNConv”的版本要好；
   L1-loss很小的版本出现了令人震惊的结果，其结果细节很好，这说明L1损失确实是结果模糊的主要原因，但是这个结果却基本做不到表情的变换，
   也就是说关键点损失(该版本的关键点损失为L1损失)没有起到任何作用。  后续的实验我想将这个“L1-loss很小的版本”的关键点损失换成判别器损失，
   探究是否“L1型的关键点损失是无用的”
3. SNResConv的版本的结果出现多个眼睛的情况，目前尚不清楚具体原因

2019/10/16:
1. 数据增强版本在训练时生成很好，但是测试时色调与实际不同，偏暗。怀疑 ①可能是旋转带来的黑边导致的 ②可能是旋转之后边角的信息丢失导致的
2. 今天的一个思考---有些判别任务不适合使用real-fake的判别，应该使用参照satrgan中分类结果的方式作为判别

2019/10/17：
1. 256-level2-rotate是用旋转90度的方式进行数据增强的版本，不过是用的”非90度旋转“的50000版本进行继续训练的，但是该版本的110000模型
   在测试集上生成的”亮度之类的颜色“不好，所幸是解决了重影问题且生成细节还可以
2. 256-level2-rotate-color是用旋转90度的方式进行数据增强并且对图像的饱和度亮度进行数据增强的版本，不过是用的”非90度旋转“的50000版本
   进行继续训练的，但是该版本的110000模型在测试集上生成的”亮度之类的颜色“很差，但是很奇怪的是在训练集上结果还可以
3. 今天想法 ① 上述两点的模型从0开始训练，看看效果  ② 将SnResConv用于ID的判别(也可考虑用做关键点kp的判别)


2019/10/18：
0. 关于昨天第3点的两个想法： ① 就算是从0开始依然存在色差问题  ② 生成效果还可以，可是依然摆脱不了重影问题
1. 今日想法，重影问题既然暂时无法用数据增强的方式解决，那么考虑到很大可能是生成器架构的问题，因此我想修改生成器从而打乱图像在网络内部的空间结构
2. 思考这么一个问题，为什么旋转90度的数据增强方式会导致颜色变化呢
3. 尝试在生成器加入self-attention,但是由于占用内存过大放弃；后来使用了Attention Augmented Conv，首先是图像大小要调整至160*160，
   其次是实际训练效果不好，于是及时停止

2019/10/19：
1. 思考卷积操作的感受域，认为现在的算法的感受域太小，因此加大了卷积核的大小，但是存在训练慢的问题，目前还在训练

2019/10/20：
1. 今天想着颜色的问题可能与归一化有关，于是试着将测试时的G.eval()去掉，发现确实几乎解决了颜色的不一致问题，并且，居然能解决重影问题。
   结果除了身份指标下降以外，其余均上升
2. 上述结果让我想探究两件事情  ① 为什么归一化的问题那么奇怪，是否是当前的instancenorm的参数不适合现有模式，因此我进行这方面实验
   ② 身份指标为何下降，接下来是应该调高身份损失的权重还是要改变训练身份判别器的方式